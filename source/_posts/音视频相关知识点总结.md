---
title: 音视频知识总结
date: {{ date }}
tags: media
cover: https://ppoffice.github.io/hexo-theme-icarus/gallery/covers/vector_landscape_2.svg
excerpt: 由于开发react-native的b站播放器项目，用到了一部分音视频的知识。因此这篇文章系统性的简述了音视频开发所需要的一些基本知识。
toc: true
---
# 音视频相关知识点总结

## 多媒体前端手册

https://www.yuque.com/webmedia/handbook/http-flv

## MSE

MSE 可以说W3C最重要的一个媒体源扩展标准。假如没有 MSE 的存在，那么什么hls.js，flv.js，dash.js都是空谈。因为 MSE，才使得媒体流能根据需要进行更精准地控制。这是w3c的文档[Media Source Extensions™](https://w3c.github.io/media-source/)。

### 介绍

早先的媒体流还是一直依赖Flash，通过rtmp或者http-flv等协议进行视频串流，再到 Flash 播放器。

然而有了MSE(media souce extensions)之后，情况就不一样了。我们可以引用`MediaSource`对象，你可以把它看成一个容器，一个引用多个`SourceBuffer`对象的容器。那么在上面我们就提到，MSE能够根据需要对媒体流进行更精准的控制，例如根据网络情况进行切码率不同的媒体流，或是根据内存占用情况释放之前播放过的媒体流等等。

这是一张Media Source的结构图：
[![img](https://lucius0.github.io/images/qiniu/180223211232.png)](https://lucius0.github.io/images/qiniu/180223211232.png)

### MediaSource

`Media Source Extensions API`的`MediaSource`接口表示`HTMLMediaElement`对象的媒体数据源。`MediaSource`对象可以附加到`HTMLMediaElement`在客户端中播放。

## 播放器的基本流程

### 1. 拉流

video元素本身支持h264编码的mp4、vp8编码的webm、theora编码的ogg视频格式。浏览器内部处理拉流逻辑。

对于像flv格式的视频流数据，我们需要自行拉取数据。

浏览器依赖 HTTP FLV 或者 WebSocket 中的一种协议来传输FLV。其中HTTP FLV需通过流式IO去拉取数据，支持流式IO的有[fetch](https://fetch.spec.whatwg.org/)或者[stream](https://streams.spec.whatwg.org/)。

相关用法如下：



#### Fetch API 

Fetch API 会在发起请求后得到的 Promise 对象中返回一个 `Response` 对象，而 `Response` 对象除了提供 `headers`、`redirect()` 等参数和方法外，还实现了 `Body` 这个 mixin 类，而在 `Body` 上我们才看到我们常用的那些 `res.json()`、`res.text()`、`res.arrayBuffer()` 等方法。在 `Body` 上还有一个 `body` 参数，这个 `body` 参数就是一个 `ReadableStream`。



```javascript
fetch(this.url, {
  method: "GET"
}).then(resp => {
  const { status, statusText } = resp;
  resp.body.getReader().then(result => {
    let { value, done } = result;
    value = new Uint8Array(value ? value : 0);
    this.data = concat(this.data, value);
    if (done) {
      this.done = true;
    } else if (this.data.length < this.chunkSize) {
      // ...,
    }
  });
});
```



#### Streams API 

Streams API 赋予了网络请求以片段处理数据的能力，过去我们使用 `XMLHttpRequest` 获取一个文件时，我们必须等待浏览器下载完整的文件，等待浏览器处理成我们需要的格式，收到所有的数据后才能处理它。现在有了流，我们可以以 `TypedArray` 片段的形式接收一部分二进制数据，然后直接对数据进行处理，这就有点像是浏览器内部接收并处理数据的逻辑。甚至我们可以将一些操作以流的形式封装，再用管道把多个流连接起来，管道的另一端就是最终处理好的数据。



```javascript
this.xhr = new XMLHttpRequest();
this.xhr.open("GET", this.url);
this.xhr.responseType = "arraybuffer";
this.xhr.setRequestHeader("Range", `bytes=${startIndex}-${endIndex}`);

this.xhr.onload = () => {
  if (this.xhr.readyState == 4) {
    if (this.xhr.status >= 200 && this.xhr.status <= 299) {
      if (!this.emitted) {
        this.emitted = true;
      }
      this.startIndex = endIndex + 1;
      resolve(new Uint8Array(this.xhr.response));
    }
  }
};

this.xhr.send();
 
```

### 2. 解封装（demux）

解封装是整个播放器当中的重要环节，它起到的承上启下的关键作用。当拿到一个封装格式的文件时，播放器要如何解析并识别？

#### mux 及 demux 基本介绍

mux为`Multiplex`的缩写，封装之意，而 demux 即为解封。

通过 muxing（混流），可以将视频流、音频流甚至是字幕流合并到一个单独的文件中，作为一个信号进行传输。等传输完毕，就可以通过 demux（分离） 将里面的视频、音频或字幕分解出来各自进行解码和播放。

在理解demux之前，需要知道一些基本知识：

**ES(Elementary Stream)**：单独的一路视频、一条音频、一个字幕或者单个的附加数据。一般来说，多媒体文件有一个视频ES、一个音频ES和一个字幕ES(字幕并不是所有视频都有)。一般ES流是以NAL的（想了解NAL不要着急，后续[解码章节](https://www.yuque.com/webmedia/handbook/decode)将为你解释）形式存在的。

#### 为什么要demux

demux是视频播放中的重要流程。

多媒体（Multimedia）是多种媒体的综合，一般包括文本，声音和图像等多种媒体形式。同样多媒体文件是一个将多种媒体封装在一起的复杂文件，在多媒体文件开始播放的时候，需要把图像、声音、字幕(可能不存在)等基本流分离出来，这个分离的行为和过程就是解封装(demux)。

而分离出来的各个基本流（ES）分别送给视频解码器、音频解码器等解码后才能得到图像声音。



Demux发生的过程：

![img](\images\1576220227314-459b2160-4c6e-4a85-bda5-0e1f81e46731.png)



#### demux的使用



宏观角度上说，解封装之后获得图像、声音、字幕(可能不存在)等基本流，而后基本流可以通过解码器进行解码。



demux图解：

![img](\images\1577439445209-ea6f9a5c-89a9-4b94-acff-a74c227e6cc1.png)

无论是何种播放器，需要播放多媒体文件时，必须先对容器封装格式进行解封装(demux)，解封后解码才可播放。

### 3. 解码

https://www.yuque.com/webmedia/handbook/decode

https://leexiao.site/#/read/mqofd59vz88/dpuxt87ivmo

#### H.264 分层概念

了解编码的基本一些概念之前，需要了解一些知识。

以H.264/AVC这一视频编码标准，系统框架分为两层：视频编码层面（Video Coding Layer，简称VCL）和网络抽象层面（Network Abstraction Layer，简称NAL）。

#### NALU是什么

NAL unit(NALU)是NAL层单元流的基本语法结构，它包含一个字节的头信息（NALU header）和一系列来自VCL的原始数据字节流（RBSP）。每个NALU是一个一定字语法元素的可变长字节字符串，包括1Byte的头信息（标识后面的RBSP数据类型），以及若干个整数字节的原始字节序列负荷（RBSP）。

![img](\images\1577431414577-dddf63fa-44fc-482c-ba33-b3d98507bb5e.png)

**NALU header:**

NALU header 主要标识后面的RBSP是什么类型的数据。

**RBSP数据**

一个NALU可以携带一个**SEI**或一个**序列参数集**或一个**图像参数集**(后面都会具体介绍这些东西是什么)。

H.264采用NALU接入可适用多种网络，提高了容错能力，比如说根据NALU header可发现丢失的是哪一个VCL单元，冗余编码图像使得基本编码图像丢失仍可得到较“粗糙”的图像。

#### NALU类型

NALU类型有很多，其中重要的有SPS、PPS和帧。

![img](\images\1577625818157-a2a56b13-ab38-4dd1-b858-ce26ddc13d6f.png)

#### 解码是什么？主要做了些什么？

通过前面的学习，我们已经了解到，NALU作为基本的NAL层传输单位，在码流中无处不在，并且不同的NALU有着不同的功能，**解码就是对基本流中的NALU中的各项类型进行拆分解析，并生成对应可播放产物**。



![img](\images\1577584819299-40fc7b9f-cf27-4dd5-877b-c7e507098653.png)



音频视频播放器没有办法直接使用基本码流播放，必须将基本码流解码成细化图像及数据(对应产物)才能输送到对应的播放器播放。



#### 解码产物

- 音频解码获得的数据是PCM采样数据，称为: sample
- 视频解码获得的数据是一幅YUV或RGB图像数据，称为: picture

### SPS和PPS在码流中的图解

正常情况下，在H.264码流中

第一个 NALU 是 SPS

第二个 NALU 是 PPS

第三个 NALU 是 IDR（即时解码器刷新，其实就是第一个I帧，后续介绍I帧时会介绍）

一般情况下，如下图所示：

![3d805eba2428da1a06f15cf874cbe021.jpg](\images\1577437192069-f2fffade-e7c1-440c-8ce2-08561689850b.jpeg)

SPS与PPS后面跟随IDR帧(IDR帧一般为视频开始解码的第一个帧)，SPS为后续所有的SS(Slice Segment)提供了公共参数。从图中同样可以看出SPS及PPS的重要性，事实上，SPS与PPS中提供的公共参数，为整个解码流程提供了十分重要的数据比如(分辨率，图像序列等等)，**如果一个码流中确实了SPS及PPS参数信息，那么该码流中的NALU是无法被解码的。**

### **IBP帧**

帧指的是视频中的单幅画面,多个帧连起来就构成了视频。而在编码视频压缩过程中，为了节省存储空间，并不会将每一帧的信息都会完完全全保留下来，会采用各种方法来减少视频数据大小,其中生成I、B、P帧就是一种编码器常采用的办法。



帧作为NAL层中的基本数据，存储在NALU当中，存储着视频的真正数据。

#### IBP帧是什么

![img](https://cdn.nlark.com/yuque/0/2019/png/198452/1577587387931-f7a33201-29b9-431e-a5b6-a36e39e9ae12.png)



如图从左到右依次解码。

第一帧是I帧，它是关键帧。I帧进行帧内预测，可以单独解码本帧的数据。

第二帧是B帧，它是向前预编码帧。它要使用一个前面的I帧或P帧和一个后面的I帧或P帧进行预测。不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。

第四帧是P帧，它是向前预测编码帧。P帧在解码过程中使用一个前面的I帧或P帧作为参考图像进行运动补偿。解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。



I帧：帧内编码帧（intra picture）,I帧通常是每个GOP（MPEG所使用的一种视频压缩技术）的第一帧，经过适度地压缩，作为随机访问的参考点可以当成静态图像。I帧可以看做一个图像经过压缩后觉得产物，I帧压缩可以得6：1的压缩比而不会产生任何可觉察的模糊现象。I帧压缩可去掉视频的空间冗余信息，下面即将介绍P帧和B帧是为了去掉时间冗余信息。



P帧：前向预测编码在帧(predictive-frame),通过将图像序列中前面已编码帧的时间冗余信息去充分去除压缩传输数据量的编码图像，也成为预测帧。



B帧：双向预测内插编码帧（bi-directionalinterpolated prediction frame）,既考虑源图像序列前面的已编码帧，又估计源图像序列后面的已编码帧之间的时间冗余信息，来压缩传输数据量的编码图像，也成为双向预测帧。



### 

### 4. 复用（Remux）

### 5. 渲染